{"cells":[{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9430,"status":"ok","timestamp":1663736559985,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"cSHB1kjMUtV0","outputId":"d2e37f73-7db8-4cf0-84e3-ad618fc79fb0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 34.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.9.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 63.5 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n"]}],"source":["# 구글 코랩에서 모델 학습 진행\n","# 그리고 코랩에서 비속어 판별 웹 서비스 진행\n","!pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1663719329586,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"r9gBUaJmVZWE"},"outputs":[],"source":["#! git clone https://github.com/ZIZUN/korean-malicious-comments-dataset.git"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123,"referenced_widgets":["76b0fbf8b244493cb4276013129b79fb","986f559107ae430fa211c2a8592ed078","f54937a426e24503b809f23e737f8092","33674b6867fd4228b24f8798ccd51583","63dd44df1b054f19b5204142fe1869e1","c0ac96b6a0c640f7a6aa83241ddb062f","493996fa4c4c451f985a06a6778b9441","47c2b8265bfb446a96619cee08de7ea9","50d70d1912cd4499a07919f564ac6a74","58ae013eefd3408fb75154f232c1c195","c4c6a11e7765458c8f66cdbedffa584e"]},"executionInfo":{"elapsed":2835,"status":"ok","timestamp":1663719332411,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"ndBnH3ExVxYE","outputId":"438fbe70-1b5b-41fe-d6e2-e7b4ba125555"},"outputs":[{"name":"stderr","output_type":"stream","text":["The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"]},{"name":"stdout","output_type":"stream","text":["Moving 0 files to the new cache system\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76b0fbf8b244493cb4276013129b79fb","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["device: cuda:0\n"]}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoTokenizer,AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(\"device:\",device)"]},{"cell_type":"markdown","metadata":{"id":"XZ1Z_L0EWRKT"},"source":["데이터셋 제작\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1663719368216,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"rBZyaDwKWPQU","outputId":"008925c6-43fb-4b93-b746-71d36d599f42"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-7040f691-3522-4c04-8c25-793b1a2194a8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>content</th>\n","      <th>lable</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>이종석 한효주 나오는 드라마 이후로 드라마 안봤다. 2년전인가?? 좀 신선했었지. ...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>씨바알..노무노무 술프노... 오늘 저녁은 꽂등심이다ㅠㅜ</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>짱깨 꺼라ㅡ패쓰</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>그들의 사생활 ~ 고인이된 설리를 위해서라도 모두 조용하길 지금 누굴 탓한다고 무슨...</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>아무리 법이 뭣같아도 무슨 자격으로 개인의 신상정보를 불특정 다수에게 공개하는지 도...</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7040f691-3522-4c04-8c25-793b1a2194a8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7040f691-3522-4c04-8c25-793b1a2194a8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7040f691-3522-4c04-8c25-793b1a2194a8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                             content  lable\n","0  이종석 한효주 나오는 드라마 이후로 드라마 안봤다. 2년전인가?? 좀 신선했었지. ...    0.0\n","1                    씨바알..노무노무 술프노... 오늘 저녁은 꽂등심이다ㅠㅜ    0.0\n","2                                           짱깨 꺼라ㅡ패쓰    0.0\n","3  그들의 사생활 ~ 고인이된 설리를 위해서라도 모두 조용하길 지금 누굴 탓한다고 무슨...    1.0\n","4  아무리 법이 뭣같아도 무슨 자격으로 개인의 신상정보를 불특정 다수에게 공개하는지 도...    1.0"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"/content/Dataset.csv\",sep=\"\\t\")\n","df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292,"status":"ok","timestamp":1663719370786,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"nrng8N8QWb3t","outputId":"e4402dd5-7213-44b1-fc76-121dbe2aad00"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 2 columns):\n"," #   Column   Non-Null Count  Dtype  \n","---  ------   --------------  -----  \n"," 0   content  10000 non-null  object \n"," 1   lable    9975 non-null   float64\n","dtypes: float64(1), object(1)\n","memory usage: 156.4+ KB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663719372015,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"O-lEAcm9We_5","outputId":"766381e8-186a-442b-c36c-11999303a9b6"},"outputs":[{"data":{"text/plain":["content     0\n","lable      25\n","dtype: int64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.isna().sum()  # 25개 결측값 확인"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663719373184,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"pbjw8ixQWoOp","outputId":"692383f0-ba3a-40ae-d79e-cc44e54fb318"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 2 columns):\n"," #   Column   Non-Null Count  Dtype  \n","---  ------   --------------  -----  \n"," 0   content  10000 non-null  object \n"," 1   lable    9975 non-null   float64\n","dtypes: float64(1), object(1)\n","memory usage: 156.4+ KB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"VXGzthhyW6aA"},"source":["전처리"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1663719375506,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"Z8g-_qh6W4H3","outputId":"2356f839-52a5-45db-8398-ceebbe11f988"},"outputs":[{"data":{"text/plain":["1602    응애 응애 엄마 저 맘에 안들죠? ........아들 ?? \" 너 내가 우스워 보이...\n","1654           토니스타크 평소 \"아이엠그루트\"라는 유행어를 부러워했다는게 학계의 정설\\t1\n","1992    \"13일 현대차에 따르면 올 들어 국내 소비자들의 수입차 구매의향률이 3년 만에 하...\n","2920                 에이프릴이 한마디 합니다 \"예쁜게 죄\" 구하라님 \"무기징역\"\\t1\n","3720          답글 글씨체를 봐라 저게 애새끼가 쓴거냐?\"빨갱이새끼가 쓴거지 ㅁㅈㅎㅉㅉ\\t0\n","3807    알겠다이기ㅋㅋ 딱 채찍쳐맞는거 좋아하는 한국식 마인드네. 노예마인드. 조금만 성공한...\n","3908           이래서 스스로 걸리거든 \"죄인들이\"~ㅎㅎㅎ 재미보고 털리고 그치~~~?\\t0\n","4241    아버지는 내재된 악마들을 다룰 정신적 힘을 가지고 있지 않았다.\" 이 말한마디가 사...\n","4283    댓글 중 \"선동 당해서 촞불든 개돼지 홍어들도 단죄를 받아야 할 공범자들이다\"에10...\n","5000    스파이 제안받고 살해 안당하는 법1. 처음에 스파이 제안을 받았을때 \"중국을 위해서...\n","5521    \"국방부 \"까지 ㅡㄱ ㅐ 엿같은 ㅈ ㅣ랄주댕이...좌빨에서 ㅡ인민군대로 ㅡ가려는건가...\n","5866    쌩뚱맞게 60대최반엌 치매라니 그것도 곱게 사는 사모님이- -\" 알콜중독도 아니고 ...\n","6477    페미메퇘지쿵쾅년인 메갈페미들은 니들이 좋아하는 싫어요 ㄱㄱ제발부탁해~~\"일반 여성\"...\n","6538    아니 ㅆㅂ 그런 \"카더라\"가 넘쳐난다고 그거에 대해서 혹시 댓글게이는 뭔가 아는거 ...\n","6771    저 때 투니버스에서 코요태 짧게 인터뷰 했었는데 김종민이 \"노래는 뭐 신지가 다 하...\n","6932               개 족 가튼 국방부의 \"휴기연장콜센터\"발족을 축하한다 ㅆ ㅂ..\\t0\n","7199    민족적 자존심과 애국심을 갖고 국산품 이용합시다 . . . \"겸손\"한 마음으로 재산...\n","7252    아나운서는 목표가 아니었지ㅋㅋ재벌하고 결혼하자마자 바로 은퇴하네ㅋㅋ무슨 인터뷰한 거...\n","7270    결국 준영과 다솜은 바람을 피게되고 무인도로 떠난다에 한표 ㅋㅋㅋ 자연인이 되어 \"...\n","7480    지금 연락하는 여자랑 폰섹 엄청 많이했는데만나서 호텔 들어가서침대에 서로 마주보고 ...\n","7499    몽골한테 \"최근에\" 250년간 지배당하고 집단강간을 당했는데 동양피가 하나도 안섞였...\n","7887    뭐 선천적으로 여성스럽거나 여자역할을 하고 싶어하는 동성애자들 그럴 수 있다고는 생...\n","9666         ㄹㅇ 시발 그냥 \"다른 진로 생각해 보세요\"라고만 했어도 욕 안처 먹었지.\\t0\n","9698                              간만에 이단어가 떠오르는군 \"이뭐병\"\\t0\n","9875    노라조 \"형\"이란 노래로 힘들 때 위로를 받곤 했습니다. 앞으로도 노라조라는 이름으...\n","Name: content, dtype: object"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["null_idx = df[df.lable.isnull()].index\n","df.loc[null_idx, \"content\"]"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663719376618,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"G4zHdAQRXEki"},"outputs":[],"source":["# label은 content 가장 끝 문자열로 설정\n","df.loc[null_idx, \"lable\"] = df.loc[null_idx, \"content\"].apply(lambda x: x[-1])\n","\n","# content \"\\t\" 앞부분까지 문자열로 설정\n","df.loc[null_idx, \"content\"] = df.loc[null_idx, \"content\"].apply(lambda x: x[-2])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663719377828,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"qV2YRdVGXdNN","outputId":"236dd996-56b6-4896-e3e4-c4c77e1789d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 2 columns):\n"," #   Column   Non-Null Count  Dtype \n","---  ------   --------------  ----- \n"," 0   content  10000 non-null  object\n"," 1   lable    10000 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 156.4+ KB\n"]}],"source":["df = df.astype({\"lable\":\"int\"}) # 학습 위해 int로 변환\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"lPN3Q5T_XwmN"},"source":["데이터셋 나누기"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663719379722,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"Ft34o3sKXmU3"},"outputs":[],"source":["train_data = df.sample(frac=0.8, random_state=42)\n","test_data = df.drop(train_data.index)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663719380806,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"kfhSg7Z4YH18","outputId":"c6fe0787-0015-490f-d6a5-f429d06b5412"},"outputs":[{"name":"stdout","output_type":"stream","text":["중복 제거 전 train: {}. 8000\n","중복 제거 전 test: {}. 2000\n","중복 제거 후 train: {}. 7972\n","중복 제거 후 test: {}. 1997\n"]}],"source":["# 데이터셋 갯수 확인\n","print('중복 제거 전 train: {}.',format(len(train_data)))\n","print('중복 제거 전 test: {}.',format(len(test_data)))\n","\n","# 중복 제거\n","train_data.drop_duplicates(subset=[\"content\"],inplace=True)\n","test_data.drop_duplicates(subset=[\"content\"],inplace=True)\n","\n","\n","# 데이터셋 갯수 확인\n","print('중복 제거 후 train: {}.',format(len(train_data)))\n","print('중복 제거 후 test: {}.',format(len(test_data)))"]},{"cell_type":"markdown","metadata":{"id":"SVcd4PbPYpSo"},"source":["토크나이징"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wLGE_AERp5-8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["3294cf310f1040318a6391c1451f8227","9b939ed60160499eb936c9fe8af11436","350e2e4af537435e9d407631efaaa949","6d7517c6c105442c9cbf81bcdbe4ca53","dd1831bcd97945a4b06605913423c1fb","5629e82457314bc4bdb8317cb07d9469","a03c22969b59449095c08deb1ed936d3","1180fdb848134175a732711542dccaa2","bed742cf473d4851877c0cf85ed6d86c","b186851d9ada4372a59c5541171eab68","ae91a8d28c6e4d7293a3aa1e789cd9ef","c0f0ae2efae54e4cacb24902b20481d1","675a0a95b7dd42c0b161c1f4c192437a","fac96b73f3d2433a89dcdc8f5506f0d3","818264f321f74f8da43001f8260a4e17","9a9fedd1edc840ba88eabd4420bb93c7","d21c953f3b094f41bcb3b0aba40add4f","437f0eecc47e4920acfc75840a44bacf","7e3a83b1de9e4d40bd05c229fd2f2aa4","5dd813f432504585893c0005534bcbd8","5675e448fa684a688a96b119429d2b25","abae7a0b8dee4f42a95e1af93f0ddec5","ae6f49d07ca94adeb937cb2839e302be","d6f370a089bf4afa8cff151879f56ff2","1639cc44222941f1859481f31cd9ae69","ed71340e537f41da9ca1e045a62886a0","6990bcdc64cd49989687089f58337e5c","31186385f7fc4afeb783a0bfd041c70c","371fd62a9a414a8a8779bb593fbcf134","6597e89a4b7045cbaca761b661c2a665","fc91046f49324acdab15195d931029e9","1fafa82d7fed48f59b769bd7168f669b","8000404794d74555bce6c976c0744067","ad794d7d80774788afa7d1004a64d55b","fac9ead8190c41498e423af0c08cba40","a5793670c76443d6a3a1f7301f3739cc","3601c5c6e01f4cd7a2a63af0a27f30db","ae12168c9b3748ec8b46f05d53986596","b63608f016da48bab481472c606bfda7","1ac4d65ebcd045d1b15c01bbc7f89a6c","cec52d6f73184822928d2f9431bba3dd","ca4536a64edd470dbe86ae36daf659c0","70cf56190b344a0aa56ff31482426dcb","cdb45e540e174557a85c1d3ce34c671d"]},"executionInfo":{"elapsed":3165,"status":"ok","timestamp":1663719386774,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"ZUenumWpYnwE","outputId":"65370085-b492-4f42-addb-07b05b1592da"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3294cf310f1040318a6391c1451f8227","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/288 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0f0ae2efae54e4cacb24902b20481d1","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/504 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae6f49d07ca94adeb937cb2839e302be","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/396k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad794d7d80774788afa7d1004a64d55b","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["MODEL_NAME = \"beomi/KcELECTRA-base\" # hugging face 에 등록된 모델\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":454,"status":"ok","timestamp":1663719387225,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"wM4CKSa-ZHKQ"},"outputs":[],"source":["#학습 데이터 토크나이징\n","tokenized_train_sentences = tokenizer(\n","    list(train_data[\"content\"]),\n","    return_tensors=\"pt\",\n","    max_length=128,\n","    padding=True,\n","    truncation=True,\n","    add_special_tokens=True,)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1663719390756,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"aXAmrRaeZY9a","outputId":"076daf87-30cb-424e-830c-ea7348a7c70d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","['[CLS]', '국방부', '~', '~', '전화로', '휴가', '##연장', '##을', '한', '병사', '##들', '몇이나', '되는지', '공개해라', '~', '어느', '훌륭한', '집안', '##의', '자제', '##분들', '##인지도', '같이', '공개해라', '~', '~', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","[2, 15556, 96, 96, 22864, 10648, 17723, 4053, 3777, 28165, 4134, 20537, 14071, 16341, 96, 8460, 13624, 9747, 4041, 12954, 9160, 14777, 8387, 16341, 96, 96, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["print(tokenized_train_sentences[0])\n","print(tokenized_train_sentences[0].tokens)\n","print(tokenized_train_sentences[0].ids)\n","print(tokenized_train_sentences[0].attention_mask)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":878,"status":"ok","timestamp":1663719392797,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"4HVJL-1zZr2t"},"outputs":[],"source":["# 테스트 데이터 토크나이징\n","tokenized_test_sentences = tokenizer(\n","    list(train_data[\"content\"]),\n","    return_tensors=\"pt\",\n","    max_length=128,\n","    padding=True,\n","    truncation=True,\n","    add_special_tokens=True,)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663719393101,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"QtdabZZoZ2QU","outputId":"e2de6ca9-881a-422f-88fc-611cf43b9c77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoding(num_tokens=128, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","['[CLS]', '국방부', '~', '~', '전화로', '휴가', '##연장', '##을', '한', '병사', '##들', '몇이나', '되는지', '공개해라', '~', '어느', '훌륭한', '집안', '##의', '자제', '##분들', '##인지도', '같이', '공개해라', '~', '~', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","[2, 15556, 96, 96, 22864, 10648, 17723, 4053, 3777, 28165, 4134, 20537, 14071, 16341, 96, 8460, 13624, 9747, 4041, 12954, 9160, 14777, 8387, 16341, 96, 96, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]}],"source":["print(tokenized_test_sentences[0])\n","print(tokenized_test_sentences[0].tokens)\n","print(tokenized_test_sentences[0].ids)\n","print(tokenized_test_sentences[0].attention_mask)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663719394124,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"FbHoQptMahnO"},"outputs":[],"source":["class CurseDataset(torch.utils.data.Dataset):\n","  def __init__(self, encodings, labels):\n","    self.encodings = encodings\n","    self.labels = labels\n","\n","  def __getitem__(self, idx):\n","    item = {key: torch.tensor(val[idx]) for key,val in self.encodings.items()}\n","    item[\"labels\"] = torch.tensor(self.labels[idx])\n","    return item\n","\n","  def __len__(self): \n","    return len(self.labels)                       "]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663719395218,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"9zdDgVvEZ8G9"},"outputs":[],"source":["train_label = train_data[\"lable\"].values\n","test_label = test_data[\"lable\"].values\n","\n","train_dataset = CurseDataset(tokenized_train_sentences, train_label)\n","test_dataset = CurseDataset(tokenized_test_sentences, test_label)"]},{"cell_type":"markdown","metadata":{"id":"K_j-n2Tsbh9S"},"source":["학습"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e89b53bbed7c43778938893ad35c48aa","5c8e7e39680b47c1b8dd19416b0e8286","166547b14a994f6eb7d02b7ab6f7f77e","dcf2c974b8ec42da98f543daba3918a0","81510a3a0b454659a050b0e2991ba9c7","c4b6a063cbf5488faef00d5d2c801e63","2b92c4917a0843ba9ba47cc392072741","fa69a37ecf064167b0b1bf77e734f4d5","fcf54184bee747c2ba1cf1082101a140","4b9c2aa454ad4084892e2c64ba29c237","718435e6255a46d697cf6d184850d2da"]},"executionInfo":{"elapsed":17169,"status":"ok","timestamp":1663719414190,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"hyNFgkp_aVs0","outputId":"592571fa-9564-4c86-a4ef-9d5d01954780"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e89b53bbed7c43778938893ad35c48aa","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/498M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n","- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at beomi/KcELECTRA-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# 모델 불러오기\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n","model.to(device)"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"elapsed":6,"status":"error","timestamp":1663727754028,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"jljYkEV-5sJo","outputId":"5239c9e9-7593-4bcf-a107-2c0b43117798"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-87-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'ElectraForSequenceClassification' object has no attribute 'summary'"]}],"source":[]},{"cell_type":"markdown","metadata":{"id":"6DRPyXKdbuQ9"},"source":["학습 파라미터 설정"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663719414191,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"hvIr0iTtbsIL"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='./',\n","    num_train_epochs=100,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=64,\n","    logging_dir='./loss',\n","    logging_steps=500,\n","    save_total_limit=2,\n",")"]},{"cell_type":"markdown","metadata":{"id":"5EZGa0PycThF"},"source":["학습 평가지표 설정\n"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663719414191,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"nypVuYNwcGpm"},"outputs":[],"source":["def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average = 'binary')\n","  acc = accuracy_score(labels,preds)\n","  return{\n","      'accuracy':acc,\n","      'f1':f1,\n","      'precision':precision,\n","      'recall':recall\n","  }"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1663719414192,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"F9UowQ4ScXJ4"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5419092,"status":"ok","timestamp":1663671916040,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"pIlDiyMjc_vl","outputId":"d084be77-8911-47dd-b54f-5c0078ad8e37"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 7972\n","  Num Epochs = 100\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 99700\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='63741' max='99700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [63741/99700 2:37:10 < 1:28:40, 6.76 it/s, Epoch 63.93/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.039900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.062700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.051300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.043900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.036000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.045700</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.034700</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.034300</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.032400</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.037100</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.014700</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.024500</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.033300</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.047300</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.025300</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.054000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.077100</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.050100</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.055300</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.043800</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.023500</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.032900</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.020500</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.022300</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.027200</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.043600</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.026400</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.031300</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.021100</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.011800</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>0.040600</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.160700</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>0.040700</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>0.057200</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>0.043200</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>0.066000</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>0.054400</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>0.062800</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>0.043900</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>0.030100</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>0.026200</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>0.040800</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>0.018800</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>0.026600</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>0.023700</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>0.022900</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>0.025700</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>0.026100</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>0.026600</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>0.035500</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>0.198300</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>0.107200</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>0.189100</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>0.102800</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>0.036600</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>0.063400</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>0.074100</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>0.042000</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>0.075500</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>0.056400</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>0.047800</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>0.030600</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>0.052300</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>0.024000</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>0.027900</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>0.038400</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>0.042200</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>0.034400</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>0.033500</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>0.030000</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>0.064300</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>0.154500</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>0.068500</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>0.050500</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>0.059100</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>0.047300</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>0.043800</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>0.041500</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>0.036000</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>0.030300</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>0.022100</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>0.017900</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>0.021400</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>0.016500</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>0.024900</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>0.013800</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>0.018700</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>0.015800</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>0.021500</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>0.011300</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>0.012400</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>0.018900</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>0.017200</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>0.026700</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>0.020600</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>0.010700</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>0.019300</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>0.029000</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>0.024500</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>0.040700</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>0.031100</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>0.021600</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>0.019500</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>0.029500</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>0.028800</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>0.023500</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>0.037700</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>0.019600</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>0.027800</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>0.023200</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>0.031200</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>0.017800</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>0.023400</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>0.030700</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>0.023300</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>0.034100</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>0.026300</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>0.059400</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>0.014300</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>0.027300</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>0.027200</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>0.037200</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>0.029800</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>0.043300</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>0.019600</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>0.054200</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>0.038800</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./checkpoint-500\n","Configuration saved in ./checkpoint-500/config.json\n","Model weights saved in ./checkpoint-500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-9000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-1000\n","Configuration saved in ./checkpoint-1000/config.json\n","Model weights saved in ./checkpoint-1000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-9500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-1500\n","Configuration saved in ./checkpoint-1500/config.json\n","Model weights saved in ./checkpoint-1500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-2000\n","Configuration saved in ./checkpoint-2000/config.json\n","Model weights saved in ./checkpoint-2000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-1000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-2500\n","Configuration saved in ./checkpoint-2500/config.json\n","Model weights saved in ./checkpoint-2500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-1500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-3000\n","Configuration saved in ./checkpoint-3000/config.json\n","Model weights saved in ./checkpoint-3000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-2000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-3500\n","Configuration saved in ./checkpoint-3500/config.json\n","Model weights saved in ./checkpoint-3500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-2500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-4000\n","Configuration saved in ./checkpoint-4000/config.json\n","Model weights saved in ./checkpoint-4000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-3000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-4500\n","Configuration saved in ./checkpoint-4500/config.json\n","Model weights saved in ./checkpoint-4500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-3500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-5000\n","Configuration saved in ./checkpoint-5000/config.json\n","Model weights saved in ./checkpoint-5000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-4000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-5500\n","Configuration saved in ./checkpoint-5500/config.json\n","Model weights saved in ./checkpoint-5500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-4500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-6000\n","Configuration saved in ./checkpoint-6000/config.json\n","Model weights saved in ./checkpoint-6000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-5000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-6500\n","Configuration saved in ./checkpoint-6500/config.json\n","Model weights saved in ./checkpoint-6500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-5500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-7000\n","Configuration saved in ./checkpoint-7000/config.json\n","Model weights saved in ./checkpoint-7000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-6000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-7500\n","Configuration saved in ./checkpoint-7500/config.json\n","Model weights saved in ./checkpoint-7500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-6500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-8000\n","Configuration saved in ./checkpoint-8000/config.json\n","Model weights saved in ./checkpoint-8000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-7000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-8500\n","Configuration saved in ./checkpoint-8500/config.json\n","Model weights saved in ./checkpoint-8500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-7500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-9000\n","Configuration saved in ./checkpoint-9000/config.json\n","Model weights saved in ./checkpoint-9000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-8000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-9500\n","Configuration saved in ./checkpoint-9500/config.json\n","Model weights saved in ./checkpoint-9500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-8500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-10000\n","Configuration saved in ./checkpoint-10000/config.json\n","Model weights saved in ./checkpoint-10000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-9000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-10500\n","Configuration saved in ./checkpoint-10500/config.json\n","Model weights saved in ./checkpoint-10500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-9500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-11000\n","Configuration saved in ./checkpoint-11000/config.json\n","Model weights saved in ./checkpoint-11000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-10000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-11500\n","Configuration saved in ./checkpoint-11500/config.json\n","Model weights saved in ./checkpoint-11500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-10500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-12000\n","Configuration saved in ./checkpoint-12000/config.json\n","Model weights saved in ./checkpoint-12000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-11000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-12500\n","Configuration saved in ./checkpoint-12500/config.json\n","Model weights saved in ./checkpoint-12500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-11500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-13000\n","Configuration saved in ./checkpoint-13000/config.json\n","Model weights saved in ./checkpoint-13000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-12000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-13500\n","Configuration saved in ./checkpoint-13500/config.json\n","Model weights saved in ./checkpoint-13500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-12500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-14000\n","Configuration saved in ./checkpoint-14000/config.json\n","Model weights saved in ./checkpoint-14000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-13000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-14500\n","Configuration saved in ./checkpoint-14500/config.json\n","Model weights saved in ./checkpoint-14500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-13500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-15000\n","Configuration saved in ./checkpoint-15000/config.json\n","Model weights saved in ./checkpoint-15000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-14000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-15500\n","Configuration saved in ./checkpoint-15500/config.json\n","Model weights saved in ./checkpoint-15500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-14500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-16000\n","Configuration saved in ./checkpoint-16000/config.json\n","Model weights saved in ./checkpoint-16000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-15000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-16500\n","Configuration saved in ./checkpoint-16500/config.json\n","Model weights saved in ./checkpoint-16500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-15500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-17000\n","Configuration saved in ./checkpoint-17000/config.json\n","Model weights saved in ./checkpoint-17000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-16000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-17500\n","Configuration saved in ./checkpoint-17500/config.json\n","Model weights saved in ./checkpoint-17500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-16500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-18000\n","Configuration saved in ./checkpoint-18000/config.json\n","Model weights saved in ./checkpoint-18000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-17000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-18500\n","Configuration saved in ./checkpoint-18500/config.json\n","Model weights saved in ./checkpoint-18500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-17500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-19000\n","Configuration saved in ./checkpoint-19000/config.json\n","Model weights saved in ./checkpoint-19000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-18000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-19500\n","Configuration saved in ./checkpoint-19500/config.json\n","Model weights saved in ./checkpoint-19500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-18500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-20000\n","Configuration saved in ./checkpoint-20000/config.json\n","Model weights saved in ./checkpoint-20000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-19000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-20500\n","Configuration saved in ./checkpoint-20500/config.json\n","Model weights saved in ./checkpoint-20500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-19500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-21000\n","Configuration saved in ./checkpoint-21000/config.json\n","Model weights saved in ./checkpoint-21000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-20000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-21500\n","Configuration saved in ./checkpoint-21500/config.json\n","Model weights saved in ./checkpoint-21500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-20500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-22000\n","Configuration saved in ./checkpoint-22000/config.json\n","Model weights saved in ./checkpoint-22000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-21000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-22500\n","Configuration saved in ./checkpoint-22500/config.json\n","Model weights saved in ./checkpoint-22500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-21500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-23000\n","Configuration saved in ./checkpoint-23000/config.json\n","Model weights saved in ./checkpoint-23000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-22000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-23500\n","Configuration saved in ./checkpoint-23500/config.json\n","Model weights saved in ./checkpoint-23500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-22500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-24000\n","Configuration saved in ./checkpoint-24000/config.json\n","Model weights saved in ./checkpoint-24000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-23000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-24500\n","Configuration saved in ./checkpoint-24500/config.json\n","Model weights saved in ./checkpoint-24500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-23500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-25000\n","Configuration saved in ./checkpoint-25000/config.json\n","Model weights saved in ./checkpoint-25000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-24000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-25500\n","Configuration saved in ./checkpoint-25500/config.json\n","Model weights saved in ./checkpoint-25500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-24500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-26000\n","Configuration saved in ./checkpoint-26000/config.json\n","Model weights saved in ./checkpoint-26000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-25000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-26500\n","Configuration saved in ./checkpoint-26500/config.json\n","Model weights saved in ./checkpoint-26500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-25500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-27000\n","Configuration saved in ./checkpoint-27000/config.json\n","Model weights saved in ./checkpoint-27000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-26000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-27500\n","Configuration saved in ./checkpoint-27500/config.json\n","Model weights saved in ./checkpoint-27500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-26500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-28000\n","Configuration saved in ./checkpoint-28000/config.json\n","Model weights saved in ./checkpoint-28000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-27000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-28500\n","Configuration saved in ./checkpoint-28500/config.json\n","Model weights saved in ./checkpoint-28500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-27500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-29000\n","Configuration saved in ./checkpoint-29000/config.json\n","Model weights saved in ./checkpoint-29000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-28000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-29500\n","Configuration saved in ./checkpoint-29500/config.json\n","Model weights saved in ./checkpoint-29500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-28500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-30000\n","Configuration saved in ./checkpoint-30000/config.json\n","Model weights saved in ./checkpoint-30000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-29000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-30500\n","Configuration saved in ./checkpoint-30500/config.json\n","Model weights saved in ./checkpoint-30500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-29500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-31000\n","Configuration saved in ./checkpoint-31000/config.json\n","Model weights saved in ./checkpoint-31000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-30000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-31500\n","Configuration saved in ./checkpoint-31500/config.json\n","Model weights saved in ./checkpoint-31500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-30500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-32000\n","Configuration saved in ./checkpoint-32000/config.json\n","Model weights saved in ./checkpoint-32000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-31000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-32500\n","Configuration saved in ./checkpoint-32500/config.json\n","Model weights saved in ./checkpoint-32500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-31500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-33000\n","Configuration saved in ./checkpoint-33000/config.json\n","Model weights saved in ./checkpoint-33000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-32000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-33500\n","Configuration saved in ./checkpoint-33500/config.json\n","Model weights saved in ./checkpoint-33500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-32500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-34000\n","Configuration saved in ./checkpoint-34000/config.json\n","Model weights saved in ./checkpoint-34000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-33000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-34500\n","Configuration saved in ./checkpoint-34500/config.json\n","Model weights saved in ./checkpoint-34500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-33500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-35000\n","Configuration saved in ./checkpoint-35000/config.json\n","Model weights saved in ./checkpoint-35000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-34000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-35500\n","Configuration saved in ./checkpoint-35500/config.json\n","Model weights saved in ./checkpoint-35500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-34500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-36000\n","Configuration saved in ./checkpoint-36000/config.json\n","Model weights saved in ./checkpoint-36000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-35000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-36500\n","Configuration saved in ./checkpoint-36500/config.json\n","Model weights saved in ./checkpoint-36500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-35500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-37000\n","Configuration saved in ./checkpoint-37000/config.json\n","Model weights saved in ./checkpoint-37000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-36000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-37500\n","Configuration saved in ./checkpoint-37500/config.json\n","Model weights saved in ./checkpoint-37500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-36500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-38000\n","Configuration saved in ./checkpoint-38000/config.json\n","Model weights saved in ./checkpoint-38000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-37000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-38500\n","Configuration saved in ./checkpoint-38500/config.json\n","Model weights saved in ./checkpoint-38500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-37500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-39000\n","Configuration saved in ./checkpoint-39000/config.json\n","Model weights saved in ./checkpoint-39000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-38000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-39500\n","Configuration saved in ./checkpoint-39500/config.json\n","Model weights saved in ./checkpoint-39500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-38500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-40000\n","Configuration saved in ./checkpoint-40000/config.json\n","Model weights saved in ./checkpoint-40000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-39000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-40500\n","Configuration saved in ./checkpoint-40500/config.json\n","Model weights saved in ./checkpoint-40500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-39500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-41000\n","Configuration saved in ./checkpoint-41000/config.json\n","Model weights saved in ./checkpoint-41000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-40000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-41500\n","Configuration saved in ./checkpoint-41500/config.json\n","Model weights saved in ./checkpoint-41500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-40500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-42000\n","Configuration saved in ./checkpoint-42000/config.json\n","Model weights saved in ./checkpoint-42000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-41000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-42500\n","Configuration saved in ./checkpoint-42500/config.json\n","Model weights saved in ./checkpoint-42500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-41500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-43000\n","Configuration saved in ./checkpoint-43000/config.json\n","Model weights saved in ./checkpoint-43000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-42000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-43500\n","Configuration saved in ./checkpoint-43500/config.json\n","Model weights saved in ./checkpoint-43500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-42500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-44000\n","Configuration saved in ./checkpoint-44000/config.json\n","Model weights saved in ./checkpoint-44000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-43000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-44500\n","Configuration saved in ./checkpoint-44500/config.json\n","Model weights saved in ./checkpoint-44500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-43500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-45000\n","Configuration saved in ./checkpoint-45000/config.json\n","Model weights saved in ./checkpoint-45000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-44000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-45500\n","Configuration saved in ./checkpoint-45500/config.json\n","Model weights saved in ./checkpoint-45500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-44500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-46000\n","Configuration saved in ./checkpoint-46000/config.json\n","Model weights saved in ./checkpoint-46000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-45000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-46500\n","Configuration saved in ./checkpoint-46500/config.json\n","Model weights saved in ./checkpoint-46500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-45500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-47000\n","Configuration saved in ./checkpoint-47000/config.json\n","Model weights saved in ./checkpoint-47000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-46000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-47500\n","Configuration saved in ./checkpoint-47500/config.json\n","Model weights saved in ./checkpoint-47500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-46500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-48000\n","Configuration saved in ./checkpoint-48000/config.json\n","Model weights saved in ./checkpoint-48000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-47000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-48500\n","Configuration saved in ./checkpoint-48500/config.json\n","Model weights saved in ./checkpoint-48500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-47500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-49000\n","Configuration saved in ./checkpoint-49000/config.json\n","Model weights saved in ./checkpoint-49000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-48000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-49500\n","Configuration saved in ./checkpoint-49500/config.json\n","Model weights saved in ./checkpoint-49500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-48500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-50000\n","Configuration saved in ./checkpoint-50000/config.json\n","Model weights saved in ./checkpoint-50000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-49000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-50500\n","Configuration saved in ./checkpoint-50500/config.json\n","Model weights saved in ./checkpoint-50500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-49500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-51000\n","Configuration saved in ./checkpoint-51000/config.json\n","Model weights saved in ./checkpoint-51000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-50000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-51500\n","Configuration saved in ./checkpoint-51500/config.json\n","Model weights saved in ./checkpoint-51500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-50500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-52000\n","Configuration saved in ./checkpoint-52000/config.json\n","Model weights saved in ./checkpoint-52000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-51000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-52500\n","Configuration saved in ./checkpoint-52500/config.json\n","Model weights saved in ./checkpoint-52500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-51500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-53000\n","Configuration saved in ./checkpoint-53000/config.json\n","Model weights saved in ./checkpoint-53000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-52000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-53500\n","Configuration saved in ./checkpoint-53500/config.json\n","Model weights saved in ./checkpoint-53500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-52500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-54000\n","Configuration saved in ./checkpoint-54000/config.json\n","Model weights saved in ./checkpoint-54000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-53000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-54500\n","Configuration saved in ./checkpoint-54500/config.json\n","Model weights saved in ./checkpoint-54500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-53500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-55000\n","Configuration saved in ./checkpoint-55000/config.json\n","Model weights saved in ./checkpoint-55000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-54000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-55500\n","Configuration saved in ./checkpoint-55500/config.json\n","Model weights saved in ./checkpoint-55500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-54500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-56000\n","Configuration saved in ./checkpoint-56000/config.json\n","Model weights saved in ./checkpoint-56000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-55000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-56500\n","Configuration saved in ./checkpoint-56500/config.json\n","Model weights saved in ./checkpoint-56500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-55500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-57000\n","Configuration saved in ./checkpoint-57000/config.json\n","Model weights saved in ./checkpoint-57000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-56000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-57500\n","Configuration saved in ./checkpoint-57500/config.json\n","Model weights saved in ./checkpoint-57500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-56500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-58000\n","Configuration saved in ./checkpoint-58000/config.json\n","Model weights saved in ./checkpoint-58000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-57000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-58500\n","Configuration saved in ./checkpoint-58500/config.json\n","Model weights saved in ./checkpoint-58500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-57500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-59000\n","Configuration saved in ./checkpoint-59000/config.json\n","Model weights saved in ./checkpoint-59000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-58000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-59500\n","Configuration saved in ./checkpoint-59500/config.json\n","Model weights saved in ./checkpoint-59500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-58500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-60000\n","Configuration saved in ./checkpoint-60000/config.json\n","Model weights saved in ./checkpoint-60000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-59000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-60500\n","Configuration saved in ./checkpoint-60500/config.json\n","Model weights saved in ./checkpoint-60500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-59500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-61000\n","Configuration saved in ./checkpoint-61000/config.json\n","Model weights saved in ./checkpoint-61000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-60000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-61500\n","Configuration saved in ./checkpoint-61500/config.json\n","Model weights saved in ./checkpoint-61500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-60500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-62000\n","Configuration saved in ./checkpoint-62000/config.json\n","Model weights saved in ./checkpoint-62000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-61000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-62500\n","Configuration saved in ./checkpoint-62500/config.json\n","Model weights saved in ./checkpoint-62500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-61500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-63000\n","Configuration saved in ./checkpoint-63000/config.json\n","Model weights saved in ./checkpoint-63000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-62000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-63500\n","Configuration saved in ./checkpoint-63500/config.json\n","Model weights saved in ./checkpoint-63500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-62500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99700' max='99700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [99700/99700 4:07:29, Epoch 100/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.039900</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.062700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.051300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.043900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.036000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.045700</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.034700</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.034300</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.032400</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.037100</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.014700</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.024500</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.033300</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.047300</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.025300</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.054000</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.077100</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.050100</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.055300</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.043800</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.023500</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.032900</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.020500</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.022300</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.027200</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.043600</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.026400</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>0.031300</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>0.021100</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>0.011800</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>0.040600</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>0.160700</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>0.040700</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>0.057200</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>0.043200</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>0.066000</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>0.054400</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>0.062800</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>0.043900</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>0.030100</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>0.026200</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>0.040800</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>0.018800</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>0.026600</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>0.023700</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>0.022900</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>0.025700</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>0.026100</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>0.026600</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>0.035500</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>0.198300</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>0.107200</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>0.189100</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>0.102800</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>0.036600</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>0.063400</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>0.074100</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>0.042000</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>0.075500</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>0.056400</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>0.047800</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>0.030600</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>0.052300</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>0.024000</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>0.027900</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>0.038400</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>0.042200</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>0.034400</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>0.033500</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>0.030000</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>0.064300</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>0.154500</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>0.068500</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>0.050500</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>0.059100</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>0.047300</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>0.043800</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>0.041500</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>0.036000</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>0.030300</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>0.022100</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>0.017900</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>0.021400</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>0.016500</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>0.024900</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>0.013800</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>0.018700</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>0.015800</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>0.021500</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>0.011300</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>0.012400</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>0.018900</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>0.017200</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>0.026700</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>0.020600</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>0.010700</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>0.019300</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>0.029000</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>0.024500</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>0.040700</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>0.031100</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>0.021600</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>0.019500</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>0.029500</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>0.028800</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>0.023500</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>0.037700</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>0.019600</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>0.027800</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>0.023200</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>0.031200</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>0.017800</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>0.023400</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>0.030700</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>0.023300</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>0.034100</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>0.026300</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>0.059400</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>0.014300</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>0.027300</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>0.027200</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>0.037200</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>0.029800</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>0.043300</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>0.019600</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>0.054200</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>0.038800</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>0.022700</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>0.023600</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>0.045100</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>0.035000</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>0.032100</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>0.063000</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>0.038900</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>0.031000</td>\n","    </tr>\n","    <tr>\n","      <td>68000</td>\n","      <td>0.036000</td>\n","    </tr>\n","    <tr>\n","      <td>68500</td>\n","      <td>0.044200</td>\n","    </tr>\n","    <tr>\n","      <td>69000</td>\n","      <td>0.026200</td>\n","    </tr>\n","    <tr>\n","      <td>69500</td>\n","      <td>0.036700</td>\n","    </tr>\n","    <tr>\n","      <td>70000</td>\n","      <td>0.027100</td>\n","    </tr>\n","    <tr>\n","      <td>70500</td>\n","      <td>0.032300</td>\n","    </tr>\n","    <tr>\n","      <td>71000</td>\n","      <td>0.024300</td>\n","    </tr>\n","    <tr>\n","      <td>71500</td>\n","      <td>0.028600</td>\n","    </tr>\n","    <tr>\n","      <td>72000</td>\n","      <td>0.027500</td>\n","    </tr>\n","    <tr>\n","      <td>72500</td>\n","      <td>0.030200</td>\n","    </tr>\n","    <tr>\n","      <td>73000</td>\n","      <td>0.020600</td>\n","    </tr>\n","    <tr>\n","      <td>73500</td>\n","      <td>0.026500</td>\n","    </tr>\n","    <tr>\n","      <td>74000</td>\n","      <td>0.039200</td>\n","    </tr>\n","    <tr>\n","      <td>74500</td>\n","      <td>0.032300</td>\n","    </tr>\n","    <tr>\n","      <td>75000</td>\n","      <td>0.028600</td>\n","    </tr>\n","    <tr>\n","      <td>75500</td>\n","      <td>0.027400</td>\n","    </tr>\n","    <tr>\n","      <td>76000</td>\n","      <td>0.032300</td>\n","    </tr>\n","    <tr>\n","      <td>76500</td>\n","      <td>0.026900</td>\n","    </tr>\n","    <tr>\n","      <td>77000</td>\n","      <td>0.019600</td>\n","    </tr>\n","    <tr>\n","      <td>77500</td>\n","      <td>0.028500</td>\n","    </tr>\n","    <tr>\n","      <td>78000</td>\n","      <td>0.035600</td>\n","    </tr>\n","    <tr>\n","      <td>78500</td>\n","      <td>0.025800</td>\n","    </tr>\n","    <tr>\n","      <td>79000</td>\n","      <td>0.025800</td>\n","    </tr>\n","    <tr>\n","      <td>79500</td>\n","      <td>0.027200</td>\n","    </tr>\n","    <tr>\n","      <td>80000</td>\n","      <td>0.028100</td>\n","    </tr>\n","    <tr>\n","      <td>80500</td>\n","      <td>0.029600</td>\n","    </tr>\n","    <tr>\n","      <td>81000</td>\n","      <td>0.019300</td>\n","    </tr>\n","    <tr>\n","      <td>81500</td>\n","      <td>0.021600</td>\n","    </tr>\n","    <tr>\n","      <td>82000</td>\n","      <td>0.031200</td>\n","    </tr>\n","    <tr>\n","      <td>82500</td>\n","      <td>0.025300</td>\n","    </tr>\n","    <tr>\n","      <td>83000</td>\n","      <td>0.018100</td>\n","    </tr>\n","    <tr>\n","      <td>83500</td>\n","      <td>0.035100</td>\n","    </tr>\n","    <tr>\n","      <td>84000</td>\n","      <td>0.021100</td>\n","    </tr>\n","    <tr>\n","      <td>84500</td>\n","      <td>0.025700</td>\n","    </tr>\n","    <tr>\n","      <td>85000</td>\n","      <td>0.032100</td>\n","    </tr>\n","    <tr>\n","      <td>85500</td>\n","      <td>0.014400</td>\n","    </tr>\n","    <tr>\n","      <td>86000</td>\n","      <td>0.014000</td>\n","    </tr>\n","    <tr>\n","      <td>86500</td>\n","      <td>0.033600</td>\n","    </tr>\n","    <tr>\n","      <td>87000</td>\n","      <td>0.016900</td>\n","    </tr>\n","    <tr>\n","      <td>87500</td>\n","      <td>0.021000</td>\n","    </tr>\n","    <tr>\n","      <td>88000</td>\n","      <td>0.021000</td>\n","    </tr>\n","    <tr>\n","      <td>88500</td>\n","      <td>0.016100</td>\n","    </tr>\n","    <tr>\n","      <td>89000</td>\n","      <td>0.025700</td>\n","    </tr>\n","    <tr>\n","      <td>89500</td>\n","      <td>0.023700</td>\n","    </tr>\n","    <tr>\n","      <td>90000</td>\n","      <td>0.021600</td>\n","    </tr>\n","    <tr>\n","      <td>90500</td>\n","      <td>0.016000</td>\n","    </tr>\n","    <tr>\n","      <td>91000</td>\n","      <td>0.019300</td>\n","    </tr>\n","    <tr>\n","      <td>91500</td>\n","      <td>0.026100</td>\n","    </tr>\n","    <tr>\n","      <td>92000</td>\n","      <td>0.012100</td>\n","    </tr>\n","    <tr>\n","      <td>92500</td>\n","      <td>0.023100</td>\n","    </tr>\n","    <tr>\n","      <td>93000</td>\n","      <td>0.021400</td>\n","    </tr>\n","    <tr>\n","      <td>93500</td>\n","      <td>0.016300</td>\n","    </tr>\n","    <tr>\n","      <td>94000</td>\n","      <td>0.018100</td>\n","    </tr>\n","    <tr>\n","      <td>94500</td>\n","      <td>0.017600</td>\n","    </tr>\n","    <tr>\n","      <td>95000</td>\n","      <td>0.030400</td>\n","    </tr>\n","    <tr>\n","      <td>95500</td>\n","      <td>0.024000</td>\n","    </tr>\n","    <tr>\n","      <td>96000</td>\n","      <td>0.017900</td>\n","    </tr>\n","    <tr>\n","      <td>96500</td>\n","      <td>0.022200</td>\n","    </tr>\n","    <tr>\n","      <td>97000</td>\n","      <td>0.025200</td>\n","    </tr>\n","    <tr>\n","      <td>97500</td>\n","      <td>0.017400</td>\n","    </tr>\n","    <tr>\n","      <td>98000</td>\n","      <td>0.024700</td>\n","    </tr>\n","    <tr>\n","      <td>98500</td>\n","      <td>0.017100</td>\n","    </tr>\n","    <tr>\n","      <td>99000</td>\n","      <td>0.024500</td>\n","    </tr>\n","    <tr>\n","      <td>99500</td>\n","      <td>0.015300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./checkpoint-64000\n","Configuration saved in ./checkpoint-64000/config.json\n","Model weights saved in ./checkpoint-64000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-63000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-64500\n","Configuration saved in ./checkpoint-64500/config.json\n","Model weights saved in ./checkpoint-64500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-63500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-65000\n","Configuration saved in ./checkpoint-65000/config.json\n","Model weights saved in ./checkpoint-65000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-64000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-65500\n","Configuration saved in ./checkpoint-65500/config.json\n","Model weights saved in ./checkpoint-65500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-64500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-66000\n","Configuration saved in ./checkpoint-66000/config.json\n","Model weights saved in ./checkpoint-66000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-65000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-66500\n","Configuration saved in ./checkpoint-66500/config.json\n","Model weights saved in ./checkpoint-66500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-65500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-67000\n","Configuration saved in ./checkpoint-67000/config.json\n","Model weights saved in ./checkpoint-67000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-66000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-67500\n","Configuration saved in ./checkpoint-67500/config.json\n","Model weights saved in ./checkpoint-67500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-66500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-68000\n","Configuration saved in ./checkpoint-68000/config.json\n","Model weights saved in ./checkpoint-68000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-67000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-68500\n","Configuration saved in ./checkpoint-68500/config.json\n","Model weights saved in ./checkpoint-68500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-67500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-69000\n","Configuration saved in ./checkpoint-69000/config.json\n","Model weights saved in ./checkpoint-69000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-68000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-69500\n","Configuration saved in ./checkpoint-69500/config.json\n","Model weights saved in ./checkpoint-69500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-68500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-70000\n","Configuration saved in ./checkpoint-70000/config.json\n","Model weights saved in ./checkpoint-70000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-69000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-70500\n","Configuration saved in ./checkpoint-70500/config.json\n","Model weights saved in ./checkpoint-70500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-69500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-71000\n","Configuration saved in ./checkpoint-71000/config.json\n","Model weights saved in ./checkpoint-71000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-70000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-71500\n","Configuration saved in ./checkpoint-71500/config.json\n","Model weights saved in ./checkpoint-71500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-70500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-72000\n","Configuration saved in ./checkpoint-72000/config.json\n","Model weights saved in ./checkpoint-72000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-71000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-72500\n","Configuration saved in ./checkpoint-72500/config.json\n","Model weights saved in ./checkpoint-72500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-71500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-73000\n","Configuration saved in ./checkpoint-73000/config.json\n","Model weights saved in ./checkpoint-73000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-72000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-73500\n","Configuration saved in ./checkpoint-73500/config.json\n","Model weights saved in ./checkpoint-73500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-72500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-74000\n","Configuration saved in ./checkpoint-74000/config.json\n","Model weights saved in ./checkpoint-74000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-73000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-74500\n","Configuration saved in ./checkpoint-74500/config.json\n","Model weights saved in ./checkpoint-74500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-73500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-75000\n","Configuration saved in ./checkpoint-75000/config.json\n","Model weights saved in ./checkpoint-75000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-74000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-75500\n","Configuration saved in ./checkpoint-75500/config.json\n","Model weights saved in ./checkpoint-75500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-74500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-76000\n","Configuration saved in ./checkpoint-76000/config.json\n","Model weights saved in ./checkpoint-76000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-75000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-76500\n","Configuration saved in ./checkpoint-76500/config.json\n","Model weights saved in ./checkpoint-76500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-75500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-77000\n","Configuration saved in ./checkpoint-77000/config.json\n","Model weights saved in ./checkpoint-77000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-76000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-77500\n","Configuration saved in ./checkpoint-77500/config.json\n","Model weights saved in ./checkpoint-77500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-76500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-78000\n","Configuration saved in ./checkpoint-78000/config.json\n","Model weights saved in ./checkpoint-78000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-77000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-78500\n","Configuration saved in ./checkpoint-78500/config.json\n","Model weights saved in ./checkpoint-78500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-77500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-79000\n","Configuration saved in ./checkpoint-79000/config.json\n","Model weights saved in ./checkpoint-79000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-78000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-79500\n","Configuration saved in ./checkpoint-79500/config.json\n","Model weights saved in ./checkpoint-79500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-78500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-80000\n","Configuration saved in ./checkpoint-80000/config.json\n","Model weights saved in ./checkpoint-80000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-79000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-80500\n","Configuration saved in ./checkpoint-80500/config.json\n","Model weights saved in ./checkpoint-80500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-79500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-81000\n","Configuration saved in ./checkpoint-81000/config.json\n","Model weights saved in ./checkpoint-81000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-80000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-81500\n","Configuration saved in ./checkpoint-81500/config.json\n","Model weights saved in ./checkpoint-81500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-80500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-82000\n","Configuration saved in ./checkpoint-82000/config.json\n","Model weights saved in ./checkpoint-82000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-81000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-82500\n","Configuration saved in ./checkpoint-82500/config.json\n","Model weights saved in ./checkpoint-82500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-81500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-83000\n","Configuration saved in ./checkpoint-83000/config.json\n","Model weights saved in ./checkpoint-83000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-82000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-83500\n","Configuration saved in ./checkpoint-83500/config.json\n","Model weights saved in ./checkpoint-83500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-82500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-84000\n","Configuration saved in ./checkpoint-84000/config.json\n","Model weights saved in ./checkpoint-84000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-83000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-84500\n","Configuration saved in ./checkpoint-84500/config.json\n","Model weights saved in ./checkpoint-84500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-83500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-85000\n","Configuration saved in ./checkpoint-85000/config.json\n","Model weights saved in ./checkpoint-85000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-84000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-85500\n","Configuration saved in ./checkpoint-85500/config.json\n","Model weights saved in ./checkpoint-85500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-84500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-86000\n","Configuration saved in ./checkpoint-86000/config.json\n","Model weights saved in ./checkpoint-86000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-85000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-86500\n","Configuration saved in ./checkpoint-86500/config.json\n","Model weights saved in ./checkpoint-86500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-85500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-87000\n","Configuration saved in ./checkpoint-87000/config.json\n","Model weights saved in ./checkpoint-87000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-86000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-87500\n","Configuration saved in ./checkpoint-87500/config.json\n","Model weights saved in ./checkpoint-87500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-86500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-88000\n","Configuration saved in ./checkpoint-88000/config.json\n","Model weights saved in ./checkpoint-88000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-87000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-88500\n","Configuration saved in ./checkpoint-88500/config.json\n","Model weights saved in ./checkpoint-88500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-87500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-89000\n","Configuration saved in ./checkpoint-89000/config.json\n","Model weights saved in ./checkpoint-89000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-88000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-89500\n","Configuration saved in ./checkpoint-89500/config.json\n","Model weights saved in ./checkpoint-89500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-88500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-90000\n","Configuration saved in ./checkpoint-90000/config.json\n","Model weights saved in ./checkpoint-90000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-89000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-90500\n","Configuration saved in ./checkpoint-90500/config.json\n","Model weights saved in ./checkpoint-90500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-89500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-91000\n","Configuration saved in ./checkpoint-91000/config.json\n","Model weights saved in ./checkpoint-91000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-90000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-91500\n","Configuration saved in ./checkpoint-91500/config.json\n","Model weights saved in ./checkpoint-91500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-90500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-92000\n","Configuration saved in ./checkpoint-92000/config.json\n","Model weights saved in ./checkpoint-92000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-91000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-92500\n","Configuration saved in ./checkpoint-92500/config.json\n","Model weights saved in ./checkpoint-92500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-91500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-93000\n","Configuration saved in ./checkpoint-93000/config.json\n","Model weights saved in ./checkpoint-93000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-92000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-93500\n","Configuration saved in ./checkpoint-93500/config.json\n","Model weights saved in ./checkpoint-93500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-92500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-94000\n","Configuration saved in ./checkpoint-94000/config.json\n","Model weights saved in ./checkpoint-94000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-93000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-94500\n","Configuration saved in ./checkpoint-94500/config.json\n","Model weights saved in ./checkpoint-94500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-93500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-95000\n","Configuration saved in ./checkpoint-95000/config.json\n","Model weights saved in ./checkpoint-95000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-94000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-95500\n","Configuration saved in ./checkpoint-95500/config.json\n","Model weights saved in ./checkpoint-95500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-94500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-96000\n","Configuration saved in ./checkpoint-96000/config.json\n","Model weights saved in ./checkpoint-96000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-95000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-96500\n","Configuration saved in ./checkpoint-96500/config.json\n","Model weights saved in ./checkpoint-96500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-95500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-97000\n","Configuration saved in ./checkpoint-97000/config.json\n","Model weights saved in ./checkpoint-97000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-96000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-97500\n","Configuration saved in ./checkpoint-97500/config.json\n","Model weights saved in ./checkpoint-97500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-96500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-98000\n","Configuration saved in ./checkpoint-98000/config.json\n","Model weights saved in ./checkpoint-98000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-97000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-98500\n","Configuration saved in ./checkpoint-98500/config.json\n","Model weights saved in ./checkpoint-98500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-97500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-99000\n","Configuration saved in ./checkpoint-99000/config.json\n","Model weights saved in ./checkpoint-99000/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-98000] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","Saving model checkpoint to ./checkpoint-99500\n","Configuration saved in ./checkpoint-99500/config.json\n","Model weights saved in ./checkpoint-99500/pytorch_model.bin\n","Deleting older checkpoint [checkpoint-98500] due to args.save_total_limit\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=99700, training_loss=0.03498959893807246, metrics={'train_runtime': 14849.5793, 'train_samples_per_second': 53.685, 'train_steps_per_second': 6.714, 'total_flos': 5.2438033333248e+16, 'train_loss': 0.03498959893807246, 'epoch': 100.0})"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18158,"status":"ok","timestamp":1663734217588,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"xdhzJ66mq_B7","outputId":"aaa048d1-0b8b-40ac-d961-4ff01cdfb107"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# 구글 드라이브 연결\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":79,"metadata":{"executionInfo":{"elapsed":2456,"status":"ok","timestamp":1663673721171,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"Xuo-NNXSRGZ0"},"outputs":[],"source":["torch.save(model, '/content/drive/MyDrive/model_100.pt')"]},{"cell_type":"markdown","metadata":{"id":"M6rAH3vMls_-"},"source":["모델평가"]},{"cell_type":"code","execution_count":72,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":7417,"status":"ok","timestamp":1663672120614,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"_8mwCtp7dDAx","outputId":"38d28007-666c-4b6b-d58d-bd0b3f57356f"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 1997\n","  Batch size = 64\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  import sys\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [32/32 00:06]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 4.015552043914795,\n"," 'eval_accuracy': 0.5032548823234853,\n"," 'eval_f1': 0.5,\n"," 'eval_precision': 0.4964964964964965,\n"," 'eval_recall': 0.5035532994923858,\n"," 'eval_runtime': 7.2333,\n"," 'eval_samples_per_second': 276.083,\n"," 'eval_steps_per_second': 4.424,\n"," 'epoch': 100.0}"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate(eval_dataset=test_dataset)"]},{"cell_type":"code","execution_count":73,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63469,"status":"ok","timestamp":1663672187776,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"PDtv3asOdbJG","outputId":"ab0fb3dc-efd2-4995-aa59-d751c3f47d99"},"outputs":[{"name":"stdout","output_type":"stream","text":["댓글을 입력 해주세요:이 멍청아\n",">> 악성ㅋ\n","\n","\n","댓글을 입력 해주세요:새끼야\n",">> 악성ㅋ\n","\n","\n","댓글을 입력 해주세요:안뇽\n",">> 악성ㅋ\n","\n","\n","댓글을 입력 해주세요:너 천사야?\n",">> 정상ㅇ\n","\n","\n","댓글을 입력 해주세요:똥냄새야\n",">> 악성ㅋ\n","\n","\n","댓글을 입력 해주세요:너 정말 착하다\n",">> 정상ㅇ\n","\n","\n","댓글을 입력 해주세요:ㅏㅇㄴ머하ㅣㅇㅁ;ㅏㅎ\n",">> 악성ㅋ\n","\n","\n","댓글을 입력 해주세요:0\n"]}],"source":["\n","def sentence_predict(sent):\n","  model.eval() # 평가\n","\n","  # 입력문장 토크나이징\n","  tokenized_sent = tokenizer(\n","      sent,\n","      return_tensors=\"pt\",\n","      truncation=True, \n","      add_special_tokens=True, \n","      max_length=128\n","      )\n","\n","  # 모델 위치 gpu이동\n","  tokenized_sent.to(device)\n","\n","  # 예측\n","  with torch.no_grad():\n","    outputs = model(\n","        input_ids=tokenized_sent[\"input_ids\"],\n","        attention_mask=tokenized_sent[\"attention_mask\"],\n","        token_type_ids=tokenized_sent[\"token_type_ids\"],\n","    )\n","\n","  # 결과\n","  logits = outputs[0]   ## 마지막 노드에서 아무런 Activation Function을 거치지 않은 값을 Logit\n","  logits = logits.detach().cpu()\n","  result = logits.argmax(-1)\n","  if result == 0:\n","    result = \">> 악성ㅋ\"\n","  elif result ==1:\n","    result= \">> 정상ㅇ\"\n","  return result\n","  \n","\n","while True:\n","  sentence = input(\"댓글을 입력 해주세요:\")\n","  if sentence == \"0\":\n","      break\n","  print(sentence_predict(sentence))\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"lf48AqaVwO64"},"source":["ngrok"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1663734373107,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"A4t0fZQCluJD","outputId":"7a0da695-2f2a-4e03-a646-66da37a2774f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/templates/index.html\n"]}],"source":["%%writefile /content/templates/index.html\n","<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","\n","    <link rel=\"stylesheet\" href=\"https://pyscript.net/latest/pyscript.css\" />\n","    <script defer src=\"https://pyscript.net/latest/pyscript.js\"></script>\n","     <py-env>\n","    - paths:\n","      - flask_ngrok_example.py\n","    </py-env>\n","</head>\n","    <title>Hello</title>\n","\n","  \n","<body>\n","<h1>욕설감지 챗봇엄나히ㅓㅇㅁㄴ험ㅇ너함어ㅣ;</h1>\n","\n"," \n","\n","</html>"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4468,"status":"ok","timestamp":1663734392279,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"0-WRiyJscmOx","outputId":"27680a1e-f717-4862-ebc6-45b7e88bbf6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}],"source":["# ngrok 설치\n","!sudo pip3 install flask-ngrok\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1663734392280,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"SKY1MqB7cof8"},"outputs":[],"source":["from google.colab import drive\n","from flask_ngrok import run_with_ngrok\n","from flask import Flask, render_template, request\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5211,"status":"ok","timestamp":1663734397820,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"ILuiCESWsMnB","outputId":"13ae9171-5f1a-4feb-84a9-3caf95dd8a72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyngrok==4.1.1\n","  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (6.0)\n","Building wheels for collected packages: pyngrok\n","  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15983 sha256=cc43a38ac48cdb7a64f060d155620010470d9a9813bcaba9b6a61ac4af4aed66\n","  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n","Successfully built pyngrok\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-4.1.1\n"]}],"source":[" !pip install pyngrok==4.1.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVl2mXOps3Jm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29448,"status":"ok","timestamp":1663734427257,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"1C4v_0VrcuFk","outputId":"0ccedb2b-52c2-4f30-cde8-488289e927ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading ngrok: 44%\n"," * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"name":"stdout","output_type":"stream","text":[" * Running on http://24b2-35-203-151-205.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]}],"source":["# app.py\n","from flask import Flask\n","from flask_ngrok import run_with_ngrok\n","\n","!ngrok authtoken 2F3a0HVVIP5nQnnOUs31NIbcHpX_4sgq8XrfRfdHwrbSuW1gv\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)  # Start ngrok when app is run\n","\n","@app.route(\"/\")\n","def hello():\n","    return render_template('index.html')\n","\n","if __name__ == '__main__':\n","    app.run()  # If address is in use, may need to terminate other sessions:\n","               # Runtime > Manage Sessions > Terminate Other Sessions\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":287,"status":"ok","timestamp":1663737408891,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"iXDiNGpNbNLZ"},"outputs":[],"source":["# 웹에 띄우기 위한 예측평가 함수 - detect.py\n","from xml.dom.minidom import Element\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","import torch\n","\n","\n","def sentence_predict(sent):\n","    pt_model = '/content/drive/MyDrive/model_100.pt'\n","    print(pt_model)\n","    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","    print(\"device:\", device)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model = torch.load(pt_model, map_location=device)\n","    print(model)\n","\n","    MODEL_NAME = \"beomi/KcELECTRA-base\"  # hugging face 에 등록된 모델\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","    model.eval()  # 평가\n","\n","    # 입력문장 토크나이징\n","    tokenized_sent = tokenizer(\n","        sent,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        add_special_tokens=True,\n","        max_length=128\n","    )\n","\n","    # 모델 위치 gpu이동\n","    tokenized_sent.to(device)\n","\n","    # 예측\n","    with torch.no_grad():\n","        outputs = model(\n","            input_ids=tokenized_sent[\"input_ids\"],\n","            attention_mask=tokenized_sent[\"attention_mask\"],\n","            token_type_ids=tokenized_sent[\"token_type_ids\"],\n","        )\n","\n","    # 결과\n","    logits = outputs[0]  ## 마지막 노드에서 아무런 Activation Function을 거치지 않은 값을 Logit\n","    logits = logits.detach().cpu()\n","    result = logits.argmax(-1)\n","    if result == 0:\n","        result = sent + \">> 악성ㅋ\"\n","\n","    elif result == 1:\n","        result = sent + \">> 정상ㅇ\"\n","\n","    return result\n","\n","# while True:\n","#   sentence = input(\"댓글을 입력 해주세요:\")\n","#   if sentence == \"0\":\n","#       break\n","#   print(sentence_predict(sentence))\n","#   print(\"\\n\")"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7297120,"status":"ok","timestamp":1663745089054,"user":{"displayName":"avery jane","userId":"16844881950893257975"},"user_tz":-540},"id":"FkwpQYkz9SXT","outputId":"80ee876b-b985-40e7-c7fb-042733f56710"},"outputs":[{"name":"stdout","output_type":"stream","text":["Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"," * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"name":"stdout","output_type":"stream","text":[" * Running on http://393a-35-203-151-205.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 05:23:41] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 05:23:41] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/model_100.pt\n","device: cuda:0\n","ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 05:23:51] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/model_100.pt\n","device: cuda:0\n","ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 06:56:40] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/model_100.pt\n","device: cuda:0\n","ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 06:57:16] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"]},{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/model_100.pt\n","device: cuda:0\n","ElectraForSequenceClassification(\n","  (electra): ElectraModel(\n","    (embeddings): ElectraEmbeddings(\n","      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): ElectraEncoder(\n","      (layer): ModuleList(\n","        (0): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): ElectraLayer(\n","          (attention): ElectraAttention(\n","            (self): ElectraSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): ElectraSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): ElectraIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): ElectraOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): ElectraClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [21/Sep/2022 06:58:02] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"]}],"source":["from flask import Flask, render_template, request\n","from flask_ngrok import run_with_ngrok\n","\n","\n","app = Flask(__name__)\n","run_with_ngrok(app)\n","\n","\n","!ngrok authtoken 2F3a0HVVIP5nQnnOUs31NIbcHpX_4sgq8XrfRfdHwrbSuW1gv\n","@app.route('/', methods=['GET', \"POST\"])\n","def main():\n","    if request.method == 'GET':\n","        return render_template(\"index.html\")\n","\n","    if request.method == 'POST':\n","        # 페이지에서 데이터 받아오기\n","        sentence = str(request.form['sentence'])\n","\n","        result = sentence_predict(sentence)\n","\n","    return render_template(\"index.html\", result=result)\n","\n","\n","if __name__ == \"__main__\":\n","    app.run()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxQwVTLPmHX7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMh37hrDrpZyXEJaNDZ/Po0","collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1180fdb848134175a732711542dccaa2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1639cc44222941f1859481f31cd9ae69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6597e89a4b7045cbaca761b661c2a665","max":396417,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc91046f49324acdab15195d931029e9","value":396417}},"166547b14a994f6eb7d02b7ab6f7f77e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa69a37ecf064167b0b1bf77e734f4d5","max":498271049,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcf54184bee747c2ba1cf1082101a140","value":498271049}},"1ac4d65ebcd045d1b15c01bbc7f89a6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fafa82d7fed48f59b769bd7168f669b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b92c4917a0843ba9ba47cc392072741":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31186385f7fc4afeb783a0bfd041c70c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3294cf310f1040318a6391c1451f8227":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b939ed60160499eb936c9fe8af11436","IPY_MODEL_350e2e4af537435e9d407631efaaa949","IPY_MODEL_6d7517c6c105442c9cbf81bcdbe4ca53"],"layout":"IPY_MODEL_dd1831bcd97945a4b06605913423c1fb"}},"33674b6867fd4228b24f8798ccd51583":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58ae013eefd3408fb75154f232c1c195","placeholder":"​","style":"IPY_MODEL_c4c6a11e7765458c8f66cdbedffa584e","value":" 0/0 [00:00&lt;?, ?it/s]"}},"350e2e4af537435e9d407631efaaa949":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1180fdb848134175a732711542dccaa2","max":288,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bed742cf473d4851877c0cf85ed6d86c","value":288}},"3601c5c6e01f4cd7a2a63af0a27f30db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70cf56190b344a0aa56ff31482426dcb","placeholder":"​","style":"IPY_MODEL_cdb45e540e174557a85c1d3ce34c671d","value":" 124/124 [00:00&lt;00:00, 4.57kB/s]"}},"371fd62a9a414a8a8779bb593fbcf134":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"437f0eecc47e4920acfc75840a44bacf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47c2b8265bfb446a96619cee08de7ea9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"493996fa4c4c451f985a06a6778b9441":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b9c2aa454ad4084892e2c64ba29c237":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50d70d1912cd4499a07919f564ac6a74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5629e82457314bc4bdb8317cb07d9469":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5675e448fa684a688a96b119429d2b25":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ae013eefd3408fb75154f232c1c195":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c8e7e39680b47c1b8dd19416b0e8286":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4b6a063cbf5488faef00d5d2c801e63","placeholder":"​","style":"IPY_MODEL_2b92c4917a0843ba9ba47cc392072741","value":"Downloading: 100%"}},"5dd813f432504585893c0005534bcbd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63dd44df1b054f19b5204142fe1869e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6597e89a4b7045cbaca761b661c2a665":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"675a0a95b7dd42c0b161c1f4c192437a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d21c953f3b094f41bcb3b0aba40add4f","placeholder":"​","style":"IPY_MODEL_437f0eecc47e4920acfc75840a44bacf","value":"Downloading: 100%"}},"6990bcdc64cd49989687089f58337e5c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d7517c6c105442c9cbf81bcdbe4ca53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b186851d9ada4372a59c5541171eab68","placeholder":"​","style":"IPY_MODEL_ae91a8d28c6e4d7293a3aa1e789cd9ef","value":" 288/288 [00:00&lt;00:00, 12.4kB/s]"}},"70cf56190b344a0aa56ff31482426dcb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"718435e6255a46d697cf6d184850d2da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76b0fbf8b244493cb4276013129b79fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_986f559107ae430fa211c2a8592ed078","IPY_MODEL_f54937a426e24503b809f23e737f8092","IPY_MODEL_33674b6867fd4228b24f8798ccd51583"],"layout":"IPY_MODEL_63dd44df1b054f19b5204142fe1869e1"}},"7e3a83b1de9e4d40bd05c229fd2f2aa4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8000404794d74555bce6c976c0744067":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81510a3a0b454659a050b0e2991ba9c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"818264f321f74f8da43001f8260a4e17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5675e448fa684a688a96b119429d2b25","placeholder":"​","style":"IPY_MODEL_abae7a0b8dee4f42a95e1af93f0ddec5","value":" 504/504 [00:00&lt;00:00, 18.9kB/s]"}},"986f559107ae430fa211c2a8592ed078":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0ac96b6a0c640f7a6aa83241ddb062f","placeholder":"​","style":"IPY_MODEL_493996fa4c4c451f985a06a6778b9441","value":""}},"9a9fedd1edc840ba88eabd4420bb93c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b939ed60160499eb936c9fe8af11436":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5629e82457314bc4bdb8317cb07d9469","placeholder":"​","style":"IPY_MODEL_a03c22969b59449095c08deb1ed936d3","value":"Downloading: 100%"}},"a03c22969b59449095c08deb1ed936d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5793670c76443d6a3a1f7301f3739cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cec52d6f73184822928d2f9431bba3dd","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca4536a64edd470dbe86ae36daf659c0","value":124}},"abae7a0b8dee4f42a95e1af93f0ddec5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ad794d7d80774788afa7d1004a64d55b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fac9ead8190c41498e423af0c08cba40","IPY_MODEL_a5793670c76443d6a3a1f7301f3739cc","IPY_MODEL_3601c5c6e01f4cd7a2a63af0a27f30db"],"layout":"IPY_MODEL_ae12168c9b3748ec8b46f05d53986596"}},"ae12168c9b3748ec8b46f05d53986596":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae6f49d07ca94adeb937cb2839e302be":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6f370a089bf4afa8cff151879f56ff2","IPY_MODEL_1639cc44222941f1859481f31cd9ae69","IPY_MODEL_ed71340e537f41da9ca1e045a62886a0"],"layout":"IPY_MODEL_6990bcdc64cd49989687089f58337e5c"}},"ae91a8d28c6e4d7293a3aa1e789cd9ef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b186851d9ada4372a59c5541171eab68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b63608f016da48bab481472c606bfda7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bed742cf473d4851877c0cf85ed6d86c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0ac96b6a0c640f7a6aa83241ddb062f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0f0ae2efae54e4cacb24902b20481d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_675a0a95b7dd42c0b161c1f4c192437a","IPY_MODEL_fac96b73f3d2433a89dcdc8f5506f0d3","IPY_MODEL_818264f321f74f8da43001f8260a4e17"],"layout":"IPY_MODEL_9a9fedd1edc840ba88eabd4420bb93c7"}},"c4b6a063cbf5488faef00d5d2c801e63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4c6a11e7765458c8f66cdbedffa584e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca4536a64edd470dbe86ae36daf659c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cdb45e540e174557a85c1d3ce34c671d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cec52d6f73184822928d2f9431bba3dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d21c953f3b094f41bcb3b0aba40add4f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6f370a089bf4afa8cff151879f56ff2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31186385f7fc4afeb783a0bfd041c70c","placeholder":"​","style":"IPY_MODEL_371fd62a9a414a8a8779bb593fbcf134","value":"Downloading: 100%"}},"dcf2c974b8ec42da98f543daba3918a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b9c2aa454ad4084892e2c64ba29c237","placeholder":"​","style":"IPY_MODEL_718435e6255a46d697cf6d184850d2da","value":" 498M/498M [00:07&lt;00:00, 70.2MB/s]"}},"dd1831bcd97945a4b06605913423c1fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e89b53bbed7c43778938893ad35c48aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c8e7e39680b47c1b8dd19416b0e8286","IPY_MODEL_166547b14a994f6eb7d02b7ab6f7f77e","IPY_MODEL_dcf2c974b8ec42da98f543daba3918a0"],"layout":"IPY_MODEL_81510a3a0b454659a050b0e2991ba9c7"}},"ed71340e537f41da9ca1e045a62886a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fafa82d7fed48f59b769bd7168f669b","placeholder":"​","style":"IPY_MODEL_8000404794d74555bce6c976c0744067","value":" 396k/396k [00:00&lt;00:00, 972kB/s]"}},"f54937a426e24503b809f23e737f8092":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47c2b8265bfb446a96619cee08de7ea9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50d70d1912cd4499a07919f564ac6a74","value":0}},"fa69a37ecf064167b0b1bf77e734f4d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac96b73f3d2433a89dcdc8f5506f0d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e3a83b1de9e4d40bd05c229fd2f2aa4","max":504,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5dd813f432504585893c0005534bcbd8","value":504}},"fac9ead8190c41498e423af0c08cba40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b63608f016da48bab481472c606bfda7","placeholder":"​","style":"IPY_MODEL_1ac4d65ebcd045d1b15c01bbc7f89a6c","value":"Downloading: 100%"}},"fc91046f49324acdab15195d931029e9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fcf54184bee747c2ba1cf1082101a140":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
